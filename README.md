# AI Portfolio â€“ Interactive Candidate Showcase

An endâ€‘toâ€‘end, minimal yet productionâ€‘ready showcase of an AI Software Engineer profile. Visitors chat with a model grounded in curated profile data, explore a CV, view skill stats, and get dynamic followâ€‘up questions â€“ all while applying pragmatic safeguards (PII handling, validation, clean architecture, JSONâ€‘structured LLM responses).

> FastAPI backend + Streamlit frontend. Clean boundaries. Humanâ€‘friendly answers. Small footprint. Easy to deploy.

---
## âœ¨ Highlights
- **Conversational Profile Chat** â€“ Answers always in firstâ€‘person voice ("I") with curated highlights.
- **Followâ€‘Up Question Suggestions** â€“ Injected after each answer (backend logic, not LLM hallucination).
- **Profile Grounding** â€“ Combines narrative markdown + structured JSON.
- **PII Policy** â€“ Regex detector; block or mask based on severity threshold (configurable).
- **Strict JSON LLM Contract** â€“ Schemaâ€‘validated responses reduce parsing surprises.
- **Separation of Concerns** â€“ Domain ports, use cases, infrastructure adapters, presentation layer.
- **Fast Startup & Lean Dependencies** â€“ No ORM, no database, fileâ€‘based profile.
- **Deploy Friendly** â€“ Works on Render (backend) + Streamlit Cloud (frontend) out of the box.

---
## ğŸ§± Architecture Overview
```
frontend/ (Streamlit) --> calls --> FastAPI /v1/chat
                                           |
                           +-------------------------------+
                           |  Application Layer            |
User Input --> Controller -> Use Case (AnswerQuestion) ----> LLM Service (OpenAI)
                           |        |                       ^
                           |        +-> PII Processing      |
                           |        +-> Follow-up selection |
                           +-------------------------------+
                                  |                
                                  v
                             Profile Repository (file-based)
```
Supporting layers:
- `domain/` â€“ Entities + errors + ports (LLM, profile repo, PII detector)
- `infrastructure/` â€“ OpenAI client + regex PII detector + file repo
- `application/` â€“ Use cases + follow-up selection
- `presentation/` â€“ FastAPI router + error handlers
- `frontend/` â€“ Streamlit pages (chat, CV, stats)

---
## âš™ï¸ Tech Stack
| Layer      | Tools |
|------------|-------|
| Backend    | FastAPI, Uvicorn, Pydantic v2, OpenAI SDK, Tenacity |
| Frontend   | Streamlit (multiâ€‘page + navigation), Requests |
| Testing    | Pytest |
| Infra Ops  | Render, Streamlit Cloud |

---
## ğŸš€ Quick Start (Local)
Backend (API):
```
cd backend
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\\Scripts\\activate
pip install -r requirements.txt
export OPENAI_API_KEY=sk-...   # set your key
uvicorn start:app --reload
```
Frontend (Streamlit):
```
cd frontend
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
export BACKEND_URL=http://127.0.0.1:8000
streamlit run streamlit_app.py
```
Visit: http://localhost:8501

---
## ğŸ§  LLM Response Contract
Backend sends a system prompt enforcing a JSON schema derived from the `Answer` Pydantic model:
```json
{
  "type": "object",
  "properties": {
    "answer": {"type": "string"},
    "highlights": {"type": "array"},
    "follow_up_questions": {"type": "array"}
  },
  "required": ["answer","highlights","follow_up_questions"]
}
```
If decoding fails, raw content is wrapped into a fallback `Answer` with empty arrays.

---
## ğŸ›£ï¸ Possible Next Steps
- Add analytics/logging (token counts, latency histograms)
- Rate limiting / API key auth layer
- Deterministic follow-up selection (avoid repeats)
- Richer skill stats (charts powered from structured profile JSON)
- Multi-model fallback or streaming responses

---
## ğŸ“„ License
MIT (see `LICENSE`).

---
## ğŸ™Œ Acknowledgements
Built as a concise demonstration of clean layering + LLM grounding with minimal overhead.

Enjoy exploring.